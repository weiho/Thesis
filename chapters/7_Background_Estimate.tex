\chapter{Background Estimate}

This chapter discusses methods used to estimate background processes the the signal. These background processes can be split up in to two categories; reducible and irreducible. Irreducible backgrounds consist of those background almost indistinguishable from our signal process namely clean high energy dielectron decays such as decays of the Z boson and the Drell-Yan (DY) spectrum. DY ($q\bar{q}~\rightarrow~Z/\gamma^{*}~\rightarrow~\ell^{-}\ell^{+}$) is the largest background process and also interferes with the signal processes. Another irreducible background also estimated is the Photon-Induced processes ($\gamma\gamma~\rightarrow~\ell^{-}\ell^{+}$) coming from the collision of two photons. Reducible backgrounds are those that can be reduced through event selection and three are included in this analysis. These reducible backgrounds consist of Top precesses, collisions creating single top quarks and $t\bar{t}$ events which decay to include two electrons; Diboson events, the creation of WW, WZ and ZZ events that decay in to two electron events; and finally Multi-jet \& W+jets events where one or more electron signature is faked by jet objects. All of these backgrounds are estimated via Monte Carlo generators except for the Multi-jet \& W+jets background which is estimated via a data-driven fake factor method. All background samples are summed together to create the full background estimate. MC samples are then scaled to the integrated luminosity of data collected in 2012 which is 20.3 fb$^{-1}$. Within the Z boson peak region (80 - 120 GeV) were it is known no new physics is found MC samples are scaled to data minus the multijet sample in order to rule out luminosity errors. This scale factor is found to be 1.048. Detailed below is the full derision of the background estimates ready for comparison to data.

\section{Monte Carlo samples}
   \label{sec:MC}

   Monte Carlo (MC)\footnote{Computer algorithm designed to simulate physical systems using random sampling to obtain results for a statistical physics theories such as quantum field field theories} samples are produced centrally within ATLAS using MC generators specific to  The generated events then undergo detector simulation using GEANT 4 \cite{Agostinelli2003250} which produces a data format identical to a readout from the ATLAS detector plus additional ``truth'' parameters from the original MC generation. The samples then undergo the same reconstruction as data events within ATHENA producing a MC sample ready to be analysed the same as data.\\


   {\bf\raggedright Drell-Yan}

   {\raggedright The Drell-Yan (DY) background is produced using the POWHEG + PYTHIA generator which is a next to leading order (NLO)\footnote{LO, NLO and NNLO refer to the complexity of feynman diagrams considered when calculating the cross section of an interaction} generation with POWHEG \cite{Alioli:2010xd,Alioli:2008gx} with event showering handled by PYTHIA 8 \cite{Sjostrand:2007gs}. The parton density function (PDF) used is CT10 \cite{Lai:2010vv}. A K-factor is then used in order to weight the cross-section from NLO to next to next to leading order (NNLO). This NNLO K-factor is derived using FEWZ \cite{Gavin:2010az} which uses the MSTW2008 NNLO PDF \cite{Martin:2009iq} from which a QCD+EW mass-dependent K-factor is obtained. The DY sample is split in to 16 MC truth dilepton mass bins with bin edges at (60, 120, 180, 250, 400, 600, 800, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000) GeV. The first bin from 60 - 120 GeV is a very high statistics sample providing a low statistical uncertainty for the region used for scaling MC in the Z boson peak.} \\


   {\bf\raggedright Photon-Induced}

   {\raggedright The Photon-Induced (PI) fraction is estimated via PYTHIA 8 \cite{Sjostrand:2007gs} generator with the LO PDF MRST2004QED \cite{Martin:2004dh}. This sample is split in to 5 dilepton mass bins with bin edges at (60, 200, 600, 1500, 2500) GeV.}\\


   {\bf\raggedright Diboson}

   {\raggedright The Diboson MC sample is produced using HERWIG 6.510 \cite{Corcella:2002jc} with the LO PDF CTEQ6L1 \cite{Pumplin:2002vw}. The sample with split in to the three process, WW, WZ and ZZ, with each process split in to 3 mass-binned samples with bin edges of (60, 400, 1000) GeV. The sample is then scaled to NLO in a mass-independent way using MCFM \cite{Campbell:2010ff} with the PDF MSTW2008 NLO \cite{Martin:2009iq}}\\

   {\bf\raggedright Top}

   {\raggedright The top sample is estimated using MC@NLO 3.41 \cite{Frixione:2008ym} with NLO PDF CT10 \cite{Lai:2010vv} to generate matrix elements with JIMMY 4.31 \cite{Butterworth:1996zw} describing parton interactions and HERWIG \cite{Corcella:2002jc} deriving the underlying event and parton showers. Both $t\bar{t}$ and single Top processes are generated in two inclusive samples. A NNLO QCD K-factor is also derived using Top++ 2.0 \cite{Czakon:2013goa,Cacciari:2011hy}. The top sample also undergoes a fit at high mass where the MC has low statistics. A dijet function ($c_{0}x^{c_{1}} x^{c_{2}\log{x}}$) is used to fit between 200 - 700~GeV then the sample is cut at 500~GeV above which the dijet fit is used.}


\subsection{MC Corrections}
   \label{sec:correc}

   Corrections are applied to MC sample due to several factors including unknown run conditions within the ATLAS detector due to MC samples being run before the LHC run as well as known inefficiencies in the reconstruction of MC events. Below are listed all of the corrections which are applied on an event by even basis during the analysis of MC samples.\\


   {\bf\raggedright Pile-up Correction}

   {\raggedright Pile-up (PU) is the number of simultaneous proton-proton interactions within an event. MC samples are produced with a broad range of PU values which then get weighted according to run conditions within the detector. PU conditions can change throughout data taking and so the PU correction is specified for a particular set of ATLAS data. The distributions MC is weighted to can be seen in figure \ref{fig:pu}.}\\

   {\bf\raggedright Vertex Position Reweighting}

   {\raggedright Vertex position hard to predict pre-run and can therefore be weighted later once run conditions are known. This correction is not widely used within ATLAS due to its minimal effect however it was found to add better data background agreement to the $\cos{\theta^{*}}$ distribution within the scaling and control region. The vertex position reweighting was found to have a minimal effect on the invariant mass distribution.}\\

   {\bf\raggedright Energy Smearing Correction}

   {\raggedright The energy smearing correction is used to better estimate the energy of electron signatures. This correction is derived from a Z peak calibration study \cite{ATLAS-CONF-2014-032} done within the ATLAS electron photon performance group and matches MC to data. These corrections provide a $\eta$ and E$_{T}$ dependent smearing value applied to electron energy before electron selection.}\\

   {\bf\raggedright Electron Efficiency Scale Factor}

   {\raggedright The electron photon performance group also identified inefficiencies in electron reconstruction and identification. These form a set of scale factors applied in bins of E$_{T}$ and $\eta$ after event selection.}\\


   {\bf\raggedright Isolation and Trigger Scale Factor}

   {\raggedright Data/MC comparison for the isolation selections and the trigger requirements highlighted minor differences between data and MC. The differences were found to be below 1\% yet a uniform scale factor accommodating this were applied after event selection for completeness.}




\section{Fake Factor Multi-Jet Estimate}
   \label{sec:FFmethod}

One of the major sources of background to di-electron signals are di-jets or electron+jets (mainly W+jets) events where one or both selected leptons are jets faking electron signatures. The method for estimating this background, described here, is a ``fake factor'' or ``matrix-method''. This is a data-driven method where electrons are selected by a tight ($N_{tight}$) and loose ($N_{loose}$) selection. The tight selection is the standard electron selection used in this analysis while the loose selection has no isolation requirement and must only pass a loose++ egamma definition (see section \ref{sec:ReconElec}) with no track matching criteria. $N_{tight}$ is therefore by design a subset of $N_{loose}$. Two more hidden values are also assigned $real$ and $fake$ referring to true source of each electron. This gives us two coefficients to determine from data.

\begin{equation} \label{eq:fakeRate}
   f~=~\frac{N^{fake}_{tight}}{N^{fake}_{loose}} \qquad \qquad r~=~\frac{N^{real}_{tight}}{N^{real}_{loose}}
\end{equation}

The fake rate $f$ denotes the probability that a $fake$ electron which passes the loose requirement also passes tight while $r$ refers to the probability that a $real$ electron which passes the loose requirement also passes the tight.
Reconstructed events are split in to two distinct groups, tight($T$), and loose while failing tight($L$), where $Tight$ is now no longer a subset of $Loose$. This allows the reconstructed events to be related to the underling truth events via a matrix of fake rates shown in equation~\ref{eq:mainFakeMatrix}.

\begin{equation} \label{eq:mainFakeMatrix}
   \begin{pmatrix}
      N_{TT} \\
      N_{TL} \\
      N_{LT} \\
      N_{LL} \\
   \end{pmatrix}
   =
   \begin{pmatrix}
      r_{1}r_{2} & r_{1}f_{2} & f_{1}r_{2} & f_{1}f_{2} \\
      r_{1}(1-r_{2}) & r_{1}(1-f_{2}) & f_{1}(1-r_{2}) & f_{1}(1-f_{2}) \\
      (1-r_{1})r_{2} & (1-r_{1})f_{2} & (1-f_{1})r_{2} & (1-f_{1})f_{2} \\
      (1-r_{1})(1-r_{2}) & (1-r_{1})(1-f_{2}) & (1-f_{1})(1-r_{2}) & (1-f_{1})(1-f_{2}) \\
   \end{pmatrix}
   \begin{pmatrix}
      N_{RR} \\
      N_{RF} \\
      N_{FR} \\
      N_{FF} \\
   \end{pmatrix}
\end{equation}

The first index in equation~\ref{eq:mainFakeMatrix} refers to the highest $p_{T}$ electron while the second index refers to the second highest $p_{T}$ electron. So $N_{LT}$ indicates the reconstructed events with highest $p_{T}$ electron only passing the $Loose$ selection while the second highest $p_{T}$ electron passes $Tight$ selection. The indices 1 and 2 refer to fake rates ($f$) and efficiencies ($r$) on leading and sub-leading electrons respectively.

The interesting part for this study is the contribution to $N_{TT}$ coming from sources other than $N_{RR}$, these can be seen in equation~\ref{eq:multijet}.

\begin{align} \label{eq:multijet}
   N^{\ell+jets}_{TT}~&=~r_{1}f_{2}N_{RF}~+~f_{1}r_{2}N_{FR} \nonumber \\
   N^{di-jets}_{TT}~&=~f_{1}f_{2}N_{FF} \nonumber \\
   N^{\ell+jets~\&~di-jets}_{TT}~&=~r_{1}f_{2}N_{RF}~+~f_{1}r_{2}N_{FR}~+~f_{1}f_{2}N_{FF} 
\end{align}

This function however contains hidden variables and so equation~\ref{eq:mainFakeMatrix} is inverted to derive a better formalism.

\begin{equation}
   \begin{pmatrix}
      N_{RR} \\
      N_{RF} \\
      N_{FR} \\
      N_{FF} \\
   \end{pmatrix}
   = \alpha
   \begin{pmatrix}
      (f_{1}-1)(f_{2}-1) & (f_{1}-1)f_{2} & f_{1}(f_{2}-1) & f_{1}f_{2} \\
      (f_{1}-1)(1-r_{2}) & (1-f_{1})r_{2} & f_{1}(1-r_{2}) & -f_{1}r_{2} \\
      (r_{1}-1)(1-f_{2}) & (1-r_{1})f_{2} & r_{1}(1-f_{2}) & -r_{1}f_{2} \\
      (1-r_{1})(1-r_{2}) & (r_{1}-1)r_{2} & r_{1}(r_{2}-1) & r_{1}r_{2} \\
   \end{pmatrix}
   \begin{pmatrix}
      N_{TT} \\
      N_{TL} \\
      N_{LT} \\
      N_{LL} \\
   \end{pmatrix}
\end{equation}
where,
\begin{equation}
   \alpha~=~\frac{1}{(r_{1}-f_{1})(r_{2}-f_{2})}
\end{equation}

The fraction of selected events with at least one fake is then given by equation \ref{eq:multijet} resulting in equation \ref{eq:mainFakeResult1}.

\begin{equation} \label{eq:mainFakeResult1}
\begin{aligned}
   N^{\ell+jets~\&~di-jets}_{TT}~&=&~\alpha r_{1}f_{2}[(f_{1}-1)(1-r_{2})N_{TT}~+~(1-f_{1})r_{2}N_{TL}~&+~f_{1}(1-r_{2})N_{LT}~-~f_{1}r_{2}N_{LL}] \\
      &~+&~\alpha f_{1}r_{2}[(r_{1}-1)(1-f_{2})N_{TT}~+~(1-r_{1})f_{2}N_{TL}~&+~r_{1}(1-f_{2})N_{LT}~-~r_{1}f_{2}N_{LL}] \\
      &~+&~\alpha f_{1}f_{2}[(1-r_{1})(1-r_{2})N_{TT}~+~(r_{1}-1)r_{2}N_{TL}~&+~r_{1}(r_{2}-1)N_{LT}~+~r_{1}r_{2}N_{LL}] 
\end{aligned}
\end{equation}

\begin{equation} \label{eq:mainFakeResult2}
\begin{aligned}
   =\alpha[r_{1}f_{2}(f_{1}-1)(1-r_{2})~+~f_{1}r_{2}(r_{1}-1)(1-f_{2})~+~f_{1}f_{2}(1-r_{1})(1-r_{2})]N_{TT} \\
   +~\alpha f_{2}r_{2}[r_{1}(1-f_{1})~+~f_{1}(1-r_{1})~+~f_{1}(r_{1}-1)]N_{TL} \\
   +~\alpha f_{1}r_{1}[f_{2}(1-r_{2})~+~r_{2}(1-f_{2})~+~f_{2}(r_{2}-1)]N_{LT} \\
   -~\alpha f_{1}f_{2}r_{1}r_{2}N_{LL}
\end{aligned}
\end{equation}

Equation \ref{eq:mainFakeResult2} shows the derived formula relating the multi-jet background to fake rates, efficiencies and four independent samples $N_{TT}$, $N_{TL}$, $N_{LT}$ and $N_{LL}$ which can be selected from data. Following are the details of this method used on the full $20~fb^{-1}$ of integrated luminosity from ATLAS's 2012 run. This method was developed centrally in the resonant analysis which the author then applied and tested for this non-resonant analysis.


\subsection{Real Electron Efficiency Estimation}

The real electron efficiency is defined in equation~\ref{eq:fakeRate} as $r~=~N^{real}_{tight}/N^{real}_{loose}$. This is determined from MC using a mass binned Drell-Yan sample. The efficiencies are found for both the leading and sub-leading electrons and binned in 8 $p_{T}$ and three eta bins of $|\eta|<1.37$ (barrel), $1.52<|\eta|<2.01$ and $2.01<|\eta|<2.47$ (endcap). The efficiency is distributed between $90$ - $96\%$ as is shown in figure \ref{fig:realEff}.

   \begin{figure}[h]
      \begin{center}
      \includegraphics[width=0.48\linewidth]{images/r1.eps}
      \includegraphics[width=0.48\linewidth]{images/r2.eps}
      \end{center}
   \caption{Real electron efficiencies obtained from Drell-Yan MC and binned in $p_{T}$ and three coarse $\eta$ bins covering the barrel and two endcap regions. Efficiencies for leading electrons are shown on the left while those for subleading electron are on the right.}
   \label{fig:realEff}
   \end{figure}



\subsection{Fake Electron Rate Estimation}

The default method selected for analysing the fake rates is a single object method selection on the jet stream data. This has the advantage of more statistics and a higher energy reach compared to methods such as using tag and probe on the egamma stream data.
An array of triggers are used for selecting suitable events with many different thresholds. 
%EF\_jX\_a4chad (where X = 25, 35, 45, 55, 80, 110, 145, 180, 220, 280, 360). 
Events are associated to groups with the lowest trigger threshold they pass as each trigger has a different prescale. Objects are selected with jet algorithms and then matched to objects in the egamma stream with a $\Delta R~<~0.1$. Two further steps are taken to suppress real electrons from W decays and real Drell-Yan events. A veto of $E_{Tmiss}~>~25~GeV$ is introduced to combat the former while events with two medium++ or loose++ electrons with $|m_{tag~\&~probe}-91~GeV|~<~20~GeV$ are vetoed to counter the real Drell-Yan.

The fake rate as defined in equation~\ref{eq:fakeRate} ($f~=~N^{fake}_{tight}/N^{fake}_{loose}$) with the $N^{fake}_{tight}$ and $N^{fake}_{loose}$ distributions selected using the standard event selection on the matched egamma objects.
Due to the different prescales of each trigger a separate set of fake rates are calculated for each trigger, these are then combined as a weighted average of all fake rates. Figure \ref{fig:fakeRates} shows the distribution of fake rates for leading and subleading fakes. These are distributed between 3 - $20\%$. This calculation of fake rates was completed in the resonant analysis group and the results were carried over to this analysis. The differences in the event selection were found to not effect the results.

   \begin{figure}[h]
      \begin{center}
      \includegraphics[width=0.48\linewidth]{images/f1.eps}
      \includegraphics[width=0.48\linewidth]{images/f2.eps}
      \end{center}
   \caption{Fake rates obtained from data and binned in $p_{T}$ and four coarse $\eta$ bins covering the barrel and three endcap regions. Fake rates for leading electrons are shown on the left while those for subleading electron are on the right.}
   \label{fig:fakeRates}
   \end{figure}




\subsection{Properties of Multi-Jet Background}

In order to compose the final sample events are organised by the distributions $N_{TT}$, $N_{TL}$, $N_{LT}$ or $N_{LL}$ and weights are applied according to each electrons $p_{T}$, $\eta$ with respect to equation~\ref{eq:mainFakeResult2} and the corresponding efficiencies and fake rates. 
Figure \ref{fig:N_dist} shows these distributions before the efficiencies and fake rates are applied to weight to the final background prediction. In addition to these steps an extra fit is then applied at low invariant mass due to contamination due to the Z boson peak. This method is not suited to predicting the Multi-Jet background in the Z boson peak region and so a fit is obtained between 120 GeV and 400 GeV and stitched from 110 GeV and below. This gives a good estimate to the integral in this region for use in scaling MC's to luminosity but is not predicted to be good at predicting other variables in this region. At high-mass statistics of the sample decline and so an additional fit is made a high mass with the lower edge of the fit varied between 425 and 600 GeV and the upper edge from 700 to 1200 GeV, with the stitching point at 500 GeV.

   \begin{figure}[h]
      \begin{center}
      \includegraphics[width=0.98\linewidth]{images/N_distributions.eps}
      \end{center}
   \caption{Distribution of $N_{TT}$, $N_{TL}$, $N_{LT}$ and $N_{LL}$ from data with no weightings applied.}
   \label{fig:N_dist}
   \end{figure}






\subsection{Other Methods and Estimation of Error}
   \label{sec:MJerror}


Two other methods and variations upon them were used to test the validity of this method as well as estimate the systematic error of this background estimation procedure. These two methods are both tag and probe measurements; one on the jet stream of data, and another on the egamma stream where the method is more an ``inverse'' tag and probe with the selection of a tag with high probability of being a jet. Variations are also made on the method by assuming $r_{1}$ and $r_{2} = 1.0$ in all cases as well as changing the definition of loose but fail tight. These variations simplify the equations slightly but the method remains the same. Variations compared to the default method used to obtain the estimation were found to vary between $\pm$ 20\% throughout the invariant mass distribution. The systematic uncertainty of the multi-jet estimate was therefore taken to be a flat $20\%$ throughout the distribution.





\section{Total Background Estimate}

The total stacked summation of all the backgrounds, after all corrections and scaling within the Z peak have been applied, can be seen in figure \ref{fig:totalBKG}.


   \begin{figure}[h]
      \begin{center}
      \includegraphics[width=0.98\linewidth]{images/invMass_bkgonly.eps}
      \end{center}
   \caption{Combined background samples scaled to data in the Z peak.}
   \label{fig:totalBKG}
   \end{figure}






